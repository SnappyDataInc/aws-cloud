#############################
# Statefulset for servers
#############################
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "{{ .Release.Name }}-server"
#  TODO: Do we need to change, for example like the one given below?
#  name: {{ template "snappydata.fullname" . }}
  labels:
    app: {{ template "snappydata.name" . }}
    chart: {{ template "snappydata.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  serviceName: "{{ .Release.Name }}-server"
  replicas: {{ .Values.servers.replicaCount | default 2 }}
  selector:
    matchLabels:
      app: "{{ .Release.Name }}-server"
  template:
    metadata:
      labels:
        app: "{{ .Release.Name }}-server"
        release: {{ .Release.Name }}
    spec:
      containers:
      - name: "{{ .Release.Name }}-server"
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        # Even servers use the same port as locator ... all run on independent pods
        # ... and, the service will either roundrobin or loadbalance
        ports:
        - containerPort: 1527
          name: jdbc
        livenessProbe:
          tcpSocket:
            port: 1527
#         initial delay intentionally kept large, as server waits(250 seconds) for locator to be available
          initialDelaySeconds: 360
        command:
          - "/bin/bash"
          - "-c"
          - >
            yum -y install nc wget && yum clean all -y;
            rm -f start;
            wget -q https://raw.githubusercontent.com/SnappyDataInc/snappy-cloud-tools/SNAP-2280/docker/start;
            chmod 744 start;
            /opt/snappydata/start server --get-ip {{ .Release.Name }}-server-public --wait-for {{ .Release.Name }}-locator-internal 10334 -locators={{ .Release.Name }}-locator-internal:10334;
        lifecycle:
          preStop:
            exec:
              command: ["/opt/snappydata/sbin/snappy-servers.sh", "stop"]
        resources:
{{ toYaml .Values.servers.resources | indent 12 }}
    {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.affinity }}
      affinity:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
    {{- end }}
        volumeMounts:
        - mountPath: "/opt/snappydata/work"
          name: snappy-disk-claim

  volumeClaimTemplates:
  - metadata:
      name: snappy-disk-claim
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
      storageClassName: standard
# A PV claim using 'standard' storage class. Will work only in Google container engine.
# In other clouds, create a PV and assign storageClassName ...